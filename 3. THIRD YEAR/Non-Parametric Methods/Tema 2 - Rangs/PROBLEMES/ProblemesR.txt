# Funció que utilitzarem per detectar les sèries d'empats i la seva llargada:
ties = function(x) {
  ti = sapply(lapply(unique(x), function(xi, x) x %in% xi, x),sum)
  result = ti[ti > 1]
  if (length(result) > 0)
    names(result) = paste("t", 1:length(result), sep = "")
  return(result)
}

# ******************************************************************************
# Problema 1
# ******************************************************************************

# Aquest problema planteja una prova d'hipòtesis unilateral:
# H0: med(entrenades) == med(control)
# H1: med(entrenades) < med(control)
# on 
entrenades = c(78, 64, 75, 45, 82)
control = c(110, 70, 53, 51)

# Totes les diferències entre Entrenades i Control:
d = outer(entrenades, control, "-")
d

# Estimació puntual de la diferència de medianes entre 'entrenades' i 'control':
median(d)

# Interval de confiança "exacte": u0.05(5,4) = 1
d = sort(d)

pos.inferior = 1 + 1  # u0.05(5,4) + 1
pos.superior = 5 * 4 - pos.inferior + 1

# Interval de confiança:
c(d[pos.inferior], d[pos.superior])

# ******************************************************************************
# Problema 2
# ******************************************************************************

resultats = data.frame(
  moment = factor( rep(c("abans", "despres"), c(12,12))),
  pes = c(
    14.4, 15.9, 14.4, 13.9, 16.6, 17.4, 18.6, 20.4, 20.4, 15.4, 15.4, 14.1,
    20.4, 22.9, 19.4, 24.4, 25.1, 20.9, 24.6, 24.4, 24.9, 19.9, 21.4, 21.4
  )
)

d = resultats$pes[resultats$moment == "abans"] - resultats$pes[resultats$moment == "despres"]
d

n = length(d)

# Per problemes numèrics es detecten menys empats dels que realment hi ha:
ties(d)
# Per evitar estrany comportament dels rangs degut a operacions en coma flotant:
d = round(d, 1)
ties(d)

# Valors absoluts dels rangs de les diferències:
abs.d = abs(d)
abs.d

# Rangs dels valors absoluts de les diferències:
r = rank(abs.d)
r

# Suma de rangs positius i negatius:
r.mes = sum(r[d > 0])
r.mes
r.menys = sum(r[d < 0])
r.menys

# Valor de l'estadístic de test:
stat.t = min(r.mes, r.menys)
stat.t
# Cal comparar-lo amb el valor crític unilateral per 0.05 de la taula:
# per n = 12 aquest valor és 17, com que stat.t = 0 < 17, rebutgem H0

# La presència d'empats podria fer relativament dubtós el resultat anterior.
# Encara que n = 12 és una mica massa baix, una possibilitat alternativa seria
# la versió assimptòtica normal del test:
# La variància de stat.t corregida pels empats seria:
ti = ties(d)
ti
s2 = (n * (n + 1) * (2*n + 1)) / 24 - sum(ti^3 - ti) / 48
s2

# Estadístic z:
z = (stat.t - n * (n + 1) / 4) / sqrt(s2)
z
# És significatiu? Cal comparar-lo amb valor crític que deixaria
# probabilitat normal 0.05 tota concentrada a una cua:
z.crit = qnorm(0.05)
z.crit

# Evidentment cal rebutjar H0:
z <= z.crit
r.mes < r.menys

# O, equivalentment, amb el p-valor:
pnorm(z)

# Estimació puntual i per intervals de confiança de la diferència
# mediana de pesos que poden aixecar:

# Possible estimador de "delta"
median(d)

# Un altre possible estimador, considerat preferible:
# Primer calculem totes les semisumes de les diferències:
outer(d, d, "+") / 2
# Equivalent a l'anterior:
semiSum = function(a, b) 0.5 * (a + b)
sSums = outer(d, d, semiSum)

# Finalment obtenim la mediana de les semisumes:
median(sSums[lower.tri(sSums, diag = TRUE)])

# Interval de confiança
# Primer calcularem l'interval bilateral...
# Diferències ordenades de menor a més gran:
d.ord = sort(d)
d.ord

# Posició de l'extrem superior de l'interval:
nu = (n + 1)/2 + 0.5 * qnorm(0.975) * sqrt(n)
nu = floor(nu + 1)
nu
# posició de l'extrem inferior de l'interval:
lambda = n - nu + 1
lambda

# Interval de confiança bilateral:
d.ord[c(lambda, nu)]

# Interval de confiança unilateral (tota la probabilitat 0.05 concentrada
# en un extrem), és el que correspon al test realitzat:
nu = (n + 1)/2 + 0.5 * qnorm(0.95) * sqrt(n)
nu = floor(nu + 1)
nu
c(-Inf, d.ord[nu])

# Via directa utilitzant la funció 'wilcox.test' de R:
wilcox.test(pes ~ moment, data = resultats, paired = TRUE, 
  alternative = "less", correct = FALSE)
# La lleugera diferència amb el resultat asimptòtic obtingut "a mà" és
# deguda a que pels problemes d'arrodoniment comentats al principi
# 'wilcox.test' calcula malament els empats, determina que hi ha una
# sola sèrie d'empats de llargada 2

# El mateix que:
x = resultats$pes[resultats$moment == "abans"]
y = resultats$pes[resultats$moment == "despres"]
wilcox.test(x, y, paired = TRUE, alternative = "less", correct = FALSE)

# O bé, donant-li directament les diferències:
wilcox.test(d, alternative = "less", correct = FALSE)
# En aquest cas el p-valor és correcte

# Amb interval de confiança inclòs:
wilcox.test(d, alternative = "less", conf.int = TRUE, correct = FALSE)

# Per tenir interval bilateral:
wilcox.test(d, conf.int = TRUE, correct = FALSE)

# ******************************************************************************
# Problemes 3 i 4
# ******************************************************************************

combn(5,2)

# Funció que calcula la "taula" del test (possibles valors de l'estadístic
# i les seves probabilitats si H0 fos certa).
# Cal donar-li una mostra qualsevol "de mostra" és a dir, dos vectors
# x1 i x2 que reflecteixin la mida de cada grup i els empats
taula = function(x1, x2) {
  u = function(indexs1) {
    r1 = sum(rngs[indexs1])
    return(r1 - const.r1)
  }
  
  x = c(x1,x2)
  rngs = rank(x)
  n = length(x)
  n1 = length(x1)
  const.r1 = n1 * (n1 + 1) / 2
  cat("rangs = ", rngs, "\n")
  combs = combn(1:n, n1)
  cat("combinacions d'índexos \n")
  print(combs)
  us = apply(combs, 2, u)
  cat("U =\n")
  print(us)
  u.vals = sort(unique(us))
  probs = matrix(c(u.vals, table(us) / ncol(combs)), nrow = 2, byrow = TRUE)
  rownames(probs) = c("u", "prob")
  cat("probs =\n")
  print(probs)
  cat("probs acumulades =\n")
  cum.probs = matrix(c(u.vals, cumsum(probs[2,])), nrow = 2, byrow = TRUE)
  rownames(cum.probs) = c("u", "prob")
  print(cum.probs)
  result = list(
    rangs = rngs,
    combs = combs,
    uvals = us,
    probs = probs,
    cumProbs = cum.probs
  )
  return(invisible(result))
}

# Taula del problema 3:
taula.probl3 = taula(1:2,3:5)
# Veiem que en una prova bilateral, a la qual rebutjaríem H0 si l'estadístic
# fos molt gran o molt petit, la mínima probabilitat d'error de tipus I
# (nivell de significació) que podríem utilitzar seria 0.1 + 0.1 = 0.2

# Ja que hem obtingut la distribució de l'estadístic sota H0, aprofitem
# per calcular la mitjana i la variància de l'estadístic de test
m = weighted.mean(taula.probl3$probs[1,], taula.probl3$probs[2,])
m
sigma2 = weighted.mean(taula.probl3$probs[1,]^2, taula.probl3$probs[2,]) - m^2
sigma2
# Que veiem que coincideixen amb les fórmules habituals respectives:
n1 = 2
n2 = 3
n1 * n2 / 2
(n1 * n2 * (n1 + n2 + 1) / 12)

# Mostres que donarien el mateix resultat (tenen la mateixa estructura: 
# cap empat)
taula(c(1,5), c(2,3,4))
taula(c(1,50), c(2,3,40))

# Taula del test pel problema 4, amb dos valors empatats als rangs 3 i 4:
taula.probl4 = taula(1:2,c(3,3,5))
m = weighted.mean(taula.probl4$probs[1,], taula.probl4$probs[2,])
m
sigma2 = weighted.mean(taula.probl4$probs[1,]^2, taula.probl4$probs[2,]) - m^2
sigma2
# Veiem que la mitjana és la mateixa encara que hi hagi empats, però
# la variància és menor,
# i no coincideix amb:
(n1 * n2 * (n1 + n2 + 1) / 12)
# Com sabem, a l'expressió anterior caldria haver-li restat:
ti = ties(c(1:2,c(3,3,5)))
n1 * n2 * sum(ti^3 - ti) / (12 * (n1 + n2) * (n1 + n2 - 1))

# Altra configuració de mostra equivalent:
taula(c(1,3),c(2,3,5))

# Proveu i penseu en:
taula(c(1,5),c(2,3,5))
taula(c(1,2),c(5,3,5))
taula(1:4,5:10)
taula(1:4, c(5,5,6:10))


# ******************************************************************************
# Problema 6
# ******************************************************************************

control = c(18, 14.5, 13.5, 12.5, 23, 24, 21, 17, 18.5, 9.5, 14)
vitamina = c(27, 34, 20.5, 29.5, 20, 28, 20, 26.5, 22, 24.5, 34, 35.5, 19)

mostra = c(control, vitamina)

n1 = length(control)
n2 = length(vitamina)
N = n1 + n2

n1 * (n1 + 1) / 2
n2 * (n2 + 1) / 2
N * (N + 1) / 2

n1 * n2
n1 * n2 * (n1 + n2 + 1)
n1 * n2 * (n1 + n2 - 1)

rangs = rank(mostra)
rangs

sum(rangs[1:n1])

d = outer(control, vitamina, "-")
d
median(d)

# u0.05(11,13) = 
ties(mostra)


# ******************************************************************************
# Problema 7
# ******************************************************************************

suc = c(8.2, 9.4, 9.6, 9.7, 10.0, 14.5, 15.2, 16.1, 17.6, 21.5)
ascorbic = c(4.2, 5.2, 5.8, 6.4, 7.0, 7.3, 10.1, 11.2, 11.3, 11.5)

rangs = rank(c(suc, ascorbic))

sum(rangs[1:10]) - 10*11/2
sum(rangs[11:20]) - 10*11/2

wilcox.test(suc, ascorbic)

wilcox.test(suc, ascorbic, paired = TRUE)

wilcox.test(suc, ascorbic, paired = TRUE, alternative = "less")

wilcox.test(suc, ascorbic, paired = TRUE, alternative = "greater")

wilcox.test(suc, ascorbic, alternative = "greater")


# ******************************************************************************
# Problema 9
# ******************************************************************************

# Creación del objeto 'peso' un objeto 'numeric' que contendrá los valores
# de los 13 pesos observados (valores de cada localidad juntos, seguidos):
peso <- c(198,186,201,190, 200,203,205,190,187, 215,187,212,190)
peso
# Creación de un objeto 'localidad' que codifica la localidad de procedencia
# para cada uno de los valores anteriores:
localidad <- factor(rep(c(1,2,3), c(4,5,4)))
# (aclaración: repetimos 1 cuatro veces, 2 cinco veces, 3 cuatro veces)
localidad

# Cálculos paso a paso:
n = length(peso)
n
# Rangos:
rangs = rank(peso)
rangs
# Suma de rangos dentro de cada nivel del factor:
sumRangsLoc = tapply(rangs, localidad, sum)
sumRangsLoc
# Tamaño muestral dentro de cada nivel:
nLoc = tapply(rangs, localidad, length)
nLoc

# Estadístico H:
h = (12 / (n * (n + 1))) * sum(sumRangsLoc^2 / nLoc) - 3 * (n + 1)
h

# Pero hay empates, dos series de valores empatados, de longitudes 2 y 3.
ties = outer(rangs, unique(rangs), "==")
ties = apply(ties, 2, sum)
ties = ties[ties != 1]
# Corrección por empates:
#h = h / (1 - ((2^3 - 2) + (3^3 - 3)) / (n^3 - n))
h = h / (1 - sum(ties^3 - ties) / (n^3 - n))
h

# Hay que compararlo con un valor crítico ji-cuadrado con 3 - 1 g.d.l.:
qchisq(0.95, 2)
# o bien:
qchisq(0.95, 2, lower.tail = FALSE)

# O calcular el p-valor:
1 - pchisq(h, 2)
# o bien:
pchisq(h, 2, lower.tail = FALSE)

# Cálculo del test de Kruskal-Wallis mediante 'kruskal.test'. Variante 1:
# primer argumento de la función: vector de valores numéricos observados
# segundo argumento: vector de la misma longitud que codifica cada nivel
# del factor a estudiar
kruskal.test(peso, localidad)


# Otra forma realizar los cálculos. Mediante una fórmula:
kruskal.test(peso ~ localidad)

# En este caso funciona gracias a que las variables "peso" y "localidad"
# existen en el área de trabajo.
# Normalmente serían columnas de un data.frame:
tribolium <- data.frame(localidad, peso)

# 'tribolium' es ahora un objeto de clase 'data.frame', con dos columnas
# la primera codifica la localidad, la segunda el peso de cada ejemplar
tribolium
tribolium[,1] # o bien tribolium[,"localidad"]
tribolium[,2] # o bien tribolium[,"peso"]

# Cálculo del test de Kruskal-Wallis. Variante 2:
# primer argumento de la función: una fórmula (el modelo lineal tal
# como se expresa en R)
# segundo argumento: donde buscará los datos de las variables de la fórmula
kruskal.test(peso ~ localidad, data = tribolium)


# En realidad los residuos de un ANOVA parecen razonablemente normales,
# el resultado de un ANOVA paramétrico habría sido:
tribolium.aov <- aov(peso ~ localidad, data = tribolium)
# Vayan pulsando 'INTRO' para ver aparecer varios gráficos de diagnóstico,
# en particular en el gràfico de probabilidad (qqplot) los resíduos aparecen
# razonablemente alineados --> sugerencia de normalidad
windows(21,21)  # ventana gráfica independiente
plot(tribolium.aov)

# En cualquier caso la conclusión del test es la misma, con p-valores
# prácticamente indénticos:
summary(tribolium.aov)



# ******************************************************************************
# Problema 10
# ******************************************************************************

rangs = c(1, 4, 8, 10, 11, 14, 18, 22,
  2, 5, 6, 20,
  3, 13, 15, 16, 21, 23, 25,
  7, 9, 12, 17, 19, 24
)

dieta = factor( rep(1:4, c(8,4,7,6)))

sumRangs = tapply( rangs, dieta, sum)
n = tapply( rangs, dieta, length)
N = length( rangs)

stat = (12/(N * (N + 1))) * sum(sumRangs^2 / n) - 3 * (N + 1)
stat

# valor crític de les taules:
qchisq(0.05, length(levels(dieta)) - 1, lower.tail = FALSE)
# p-valor:
pchisq(stat, length(levels(dieta)) - 1, lower.tail = FALSE)


kruskal.test(rangs, dieta)


# ******************************************************************************
# Problema 11
# ******************************************************************************

suavitat = c(
  38.7, 41.5, 43.8, 44.5, 45.5, 46, 47.7, 58,
  39.2, 39.3, 39.7, 41.4, 41.8, 42.9, 43.3, 45.8,
  34, 35, 39, 40, 43, 43, 44, 45,
  34, 34.8, 34.8, 35.4, 37.2, 37.8, 41.2, 42.8
)

laboratori = factor(rep(c("A","B","C","D"), rep(8,4)))
unitat = factor(rep(1:8, 4))

N = length(suavitat)
N
ni = tapply(suavitat, laboratori, length)
ni

rsuavitat = rank(suavitat)
rsuavitat
ri. = tapply(rsuavitat, laboratori, sum)
ri.

h = (12 / (N * (N + 1))) * sum((ri.^2) / ni) - 3 * (N + 1)
h

ti = ties(rsuavitat)
ti

correc = 1 - sum(ti^3 - ti) / (N^3 - N)
h/correc

kruskal.test(suavitat ~ laboratori)
kruskal.test(suavitat, laboratori)
dades = data.frame(laboratori, unitat, suavitat)
kruskal.test(suavitat ~ laboratori, data = dades)

# El que hem fet anteriorment amb rangs, té el seu equivalent
# en estadística paramètrica-normal en la prova ANOVA d'un factor:
oneway.test(suavitat ~ laboratori, var.equal = TRUE)

# Part b del problema
# Típic disseny "en blocs aletoritzats"
# Sota un enfoc paramètric-normal ho resoldríem així:
# (Recordem:)
dades = data.frame(laboratori, unitat, suavitat)
anova(aov(suavitat ~ laboratori + unitat, data = dades))

friedman.test(suavitat, laboratori, unitat)
friedman.test(suavitat ~ laboratori | unitat)
friedman.test(suavitat ~ laboratori | unitat, data = dades)

n = nlevels(unitat)
s = nlevels(laboratori)

dades = matrix(suavitat, nrow = n, ncol = s)
colnames(dades) = levels(laboratori)
dades

friedman.test(dades)

# Càlcul pas a pas de l'estadístic de Friedman:
# Ara establim els rangs per separat dins cada bloc:
rsuavitat = apply(dades, 1, rank)
rsuavitat
# I sumem els rangs de cada laboratori:
ri. = rowSums(rsuavitat)
ri.
estad.q = (12 / (n * s * (s + 1))) * sum(ri.^2) - 3 * n * (s + 1)
estad.q

# Com que dins algunes unitats hi ha empats, cal corregir:
# Empats a cada unitat:
unitat.ti = apply(rsuavitat, 2, ties)
unitat.ti
correc = 1 - (1 / (n * s * (s^2 - 1))) * sum(vapply(unitat.ti, function(ti) sum(ti^3 - ti), FUN.VALUE = 0))
correc
# Valor final de l'estadístic de Friedman, corregit per empats:
estad.q / correc

# Amb la prova de Friedman hem determinat que hi ha "alguna diferència"
# En una anàlisi post-hoc provaríem de determinar exactament quines medianes són diferents
# Per comparacions dos a dos, la prova més compatible amb la de Friedman que acabem de fer
# és la de Wilcoxon dels rangs amb signe:
p.vals = numeric(s * (s - 1) / 2)

nomLab = colnames(dades)
comparacio = character(length(p.vals))
k = 0
for (i in 2:s)
  for (j in 1:(i-1)) {
    k = k + 1
    comparacio[k] = paste(nomLab[i], nomLab[j], sep ="-")
    p.vals[k] = wilcox.test(dades[,i], dades[,j], 
                            paired = TRUE, correct = FALSE)$p.value
  }

names(p.vals) = comparacio
p.vals

# Ajust per multiplicitat de tests:
p.adjust(p.vals, method = "holm")



# ******************************************************************************
# Problema 12
# ******************************************************************************
# Kendall - Spearman (cas sense empats)
# ******************************************************************************
# Dades agafades del problema 2, pesos que eren capaços d'aixecar 12 voluntaris, 
# abans (x) i després (y) de programa d'entrenament ("lleugerament" retocades per que
# no hi hagi empats)
x = c(14.4, 15.9, 14.5, 13.9, 16.6, 17.4, 18.6, 20.3, 20.4, 15.4, 15.5, 14.1)
y = c(20.4, 22.9, 19.4, 24.3, 25.1, 20.9, 24.6, 24.4, 24.9, 19.9, 21.5, 21.4)

# Mida mostral:
n = length(x)

# Taula amb totes les possibles diferències entre x[i] i x[j]:
difs.x = outer(x,x, "-")
# Descartem les diferències de la diagonal (i == j) i de la meitat triangular superior:
difs.x = difs.x[ltri <- lower.tri(difs.x)]
# Totes les possibles diferències entre y[i] i y[j]:
difs.y = outer(y,y, "-")[ltri]


# Versió alternativa del codi anterior fent servir 'for'
# (és unes 6 vegades més lenta que utilitzant 'outer')
#difs.x = difs.y = numeric(n * (n - 1) / 2)
#k = 0
#for (i in 1:n) {
#  if (i == n) break
#  for (j in (i+1):n) {
#    k = k + 1
#    difs.x[k] = x[i] - x[j]
#    difs.y[k] = y[i] - y[j]
#  }
#}

# Total de diferències concordants i discordants:
concor = sum(sign(difs.x)*sign(difs.y) > 0)
discor = sum(sign(difs.x)*sign(difs.y) < 0)

# Estadístic de Kendall:
(concor - discor) / length(difs.x)

# Versió alternativa de l'estadístic de Kendall
sum(sign(difs.x)*sign(difs.y)) / length(difs.x)

# Càlcul directe amb la funció 'cor' de R:
cor(x,y, method = "kendall")
# Comparació amb la correlació de Pearson:
cor(x,y)

# Forma directa R per fer la prova de significació de la correlació de Kendall
# H0: tau = 0   vs   H1: tau > 0
cor.test(x,y, method = "kendall", alternative = "greater", exact = TRUE)

# La mateixa prova de significació feta a partir de les taules:
# Caldria haver calculat la correlació mostral de Kendall: 0.3939394.
# Buscaríem a la taula de valors crítics per la tau de Kendall, "One tailed",
# per 0.05 i n = 12 trobaríem el valor 0.394, just coincident amb el
# valor mostral si l'arrodonim a tres decimals. De fet pel caràcter discret
# d'aquest estimador, la veritable probabilitat d'error de tipus I associada
# a aquest valor no és exactament 0.05 sinó lleugerament inferior, podem
# perfectament rebutjar H0.
# 

# L'índex de Kendall és un estadístic basat en rangs, solament utilitza la
# informació continguda als rangs, el resultat és el mateix amb les dades
# originals i amb els seus rangs:
rX = rank(x)
rY = rank(y)

cor(rX,rY, method = "kendall")

# Correlació de Spearman:
# La calcularem com la correlació mostral de Pearson sobre els rangs:
cor(rX, rY)
cor(x, y, method = "spearman")

# Càlcul de la correlació de Spearman a partir de la fórmula directa:
(12 / (n * (n^2 - 1))) * sum(rX * rY) - 3 * (n + 1) / (n - 1)

# Prova de significació:
cor.test(x, y, method = "spearman", alternative = "greater")
# Que no és el mateix que fer:
cor.test(rX, rY, alternative = "greater")
# Fixem-nos que pel primer cas ha utilitzat la prova de significació
# no paramétrica, basada en rangs, pel coeficient de Spearman.
# En canvi, pel segon cas ha fet (possiblement no del tot adequadament)
# la prova paramètrica basada en la suposició de normalitat bivariant,
# sobre les dades de rangs.


# ************************************************************************
# Problema 13
# Cas amb empats: dades "Law School"
# ************************************************************************
proves = read.table("Law_school.txt", header = TRUE)

n = nrow(proves)

# Càlcul de totes les possibles diferències:
difs.x = difs.y = numeric(n * (n - 1) / 2)
k = 0
for (i in 1:n) {
  if (i == n) break
  for (j in (i+1):n) {
    k = k + 1
    difs.x[k] = proves$LSAT[j] - proves$LSAT[i]
    difs.y[k] = proves$GPA[j] - proves$GPA[i]
  }
}

# Versió més ràpida, utilitzant la funció 'outer':
difs.x = outer(proves$LSAT, proves$LSAT, "-")
difs.x = difs.x[ltri <- lower.tri(difs.x)]
difs.y = outer(proves$GPA, proves$GPA, "-")[ltri]

# Determinació de totes les sèries d'empats i la seva llargada per 
# un vector qualsevol 'x':
ties = function(x) {
  ti = sapply(lapply(unique(x), function(xi, x) x %in% xi, x),sum)
  result = ti[ti > 1]
  if (length(result) > 0)
    names(result) = paste("t", 1:length(result), sep = "")
  return(result)
}

# Sèries d'empats i llargada de cada sèrie, per x i y:
ti = ties(proves$LSAT)
ti
ui = ties(proves$GPA)
ui

n = length(proves$LSAT)
N = length(difs.x) # = n(n - 1) / 2

# Total de diferències concordants i discordants:
concor = sum(sign(difs.x)*sign(difs.y) > 0)
discor = sum(sign(difs.x)*sign(difs.y) < 0)

(concor + discor) / N

# Tau-a:
(concor - discor) / length(difs.x)
# Tau-b:
(concor - discor) / 
  sqrt((N - 0.5 * sum(ti*(ti-1))) * (N - 0.5 * sum(ui*(ui-1))))
# Tau-c:
k = min(n - sum(ti), n - sum(ui))
2 * k * (concor - discor) / (n * n * (k - 1))

# Tau-a:
sum(sign(difs.x)*sign(difs.y)) / N
# Tau-b:
sum(sign(difs.x)*sign(difs.y)) / 
  sqrt((N - 0.5 * sum(ti*(ti-1))) * (N - 0.5 * sum(ui*(ui-1))))
# Tau-c:
k = min(n - sum(ti), n - sum(ui))
2 * k * sum(sign(difs.x)*sign(difs.y)) / (n * n * (k - 1))


# Per defecte 'cor' calcula Tau-b quan hi ha empats:
cor(proves, method = "kendall")
cor(proves$LSAT, proves$GPA, method = "kendall")

cor.test(proves$LSAT, proves$GPA, method = "kendall")

rLSAT = rank(proves$LSAT)
rGPA  = rank(proves$GPA)

cor.test(proves$LSAT, proves$GPA, method = "kendall")
cor.test(rLSAT, rGPA, method = "kendall")

require(Kendall)

Kendall(proves$LSAT, proves$GPA)

# Correlació de Spearman:
# La calcularem com la correlació mostral de Pearson sobre els rangs:
cor(rLSAT, rGPA, method = "pearson")
# o directament:
cor(proves$LSAT, proves$GPA, method = "spearman")

# Significació:
cor.test(proves$LSAT, proves$GPA, method = "spearman")
# Que no és el mateix que fer la prova t de Student basada en la suposició
# de normalitat bivariant sobre els rangs:
cor.test(rLSAT, rGPA, method = "pearson")

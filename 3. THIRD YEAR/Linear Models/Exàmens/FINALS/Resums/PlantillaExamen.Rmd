---
title: "Examen"
author: "Roger Devesa"
output: html_document
---


###ESCRIURE EL MODEL. Escriure el model de la forma Y=XB+e

  y1       x11 x12 x13              e1
  y2       x21 x22 x23     b1       e2
( y3 ) = ( x31 x32 x33 ) ( b2 ) + ( e3 )
  y4       x41 x42 x43     b3       e4
  y5       x51 x52 x53              e5

```{r,eval=FALSE}
#les xij s'han de substituir per 0 o 1 depenent del anunciat (practicar)
```


###FUNCIÓ PARAMÈTRICA ESTIMABLE. Veure si una funció és paramètrica estimable
```{r,eval=FALSE}
#cal ampliarho si tenim més vectors.

v1 <- (c(2,1,0))  #Cada posició del vector correspon a una beta (o \alpha, \beta, \gamma ,...)
v2 <- (c(1,-1,-1))
v3 <- (c(3,0,-1))
v4 <- (c(1,2,1))

#les observacions són el nombre de vectors  

n <- 3 #numero de  rèpliques si no posa nombre de repliques,llavors n=1
```

```{r,eval=FALSE}
X <- c(rep(v1,n),rep(v2,n),rep(v3,n),rep(v4,n)) #cal ampliarho si tenim més vectors ( afecgir per exemple ,rep(v5,n) )
X <- matrix(X,ncol=n,byrow=T)
r <- qr(X)$rank #el rang ens diu el nombre de vectors linealment independents.

#hem de fer eliminació gaussiana on cada fila de la matriu correspont als coeficients de una de les betas) (practicar)
```
veiem que el rang és X, això implica que només hi han X vectors independents
és a dir, (n_vectors - X) paràmetre/s serà/n una combinació lineal d'un o els altres vestors.

```{r,eval=FALSE}
#si el rang és màxim tota funció es parametrica estimable.
```
dit això, resolent el sistema trobem que $1\beta_1 + 3\beta_2 = -1\beta_3$

```{r,eval=FALSE}
#canviar per la relació trobada 1\beta_1 + 3\beta_2 = -1\beta_3
#per mirar si les funcions son apramètriques estimables mirem si els coficients de les betas (o \alpha , \beta , \gamma ,...) compleixem la condicio trobada
```


###ESTIMACIÓ BETAS.Estimació de les betas
```{r,eval=FALSE}
Y <- c(17.03, 17.01, 16.99,1.95, 2.01, 1.98,17.88, 17.97, 18.01,15.02, 15.10, 14.98) #subtituir pels valors de y que donen, el vector pot ser mes petit concretament ha de tenir tats valors com n*(numero de vectors (o nombre d'obsevacions)) (els tres primers valors corresponen a les tres repliques(resultats) trobats amb la combinació de betas feta a partir del vector 1 (v1))

n <- length(Y)
XtX <- t(X) %*% X

library(MASS) #llibreria per fer la g-inversa (ginv)

XtX.ginv <- ginv(XtX)

betas <- XtX.ginv %*% t(X) %*% Y #equacio per trobar les betas amb el mètode dels mínims cuadrats (MC)
betas #són les estimacions de les betas
```

###ESTIMACIONS MINIMO-CUADRÀTIQUES
```{r,eval=FALSE}
#en el cas que haguem de trobar les estimacions minimo-quadràtiques substituim els valors de les betas corresponents a les funcions paramètriques que hem trobat que són estimables.
```


###MATRIU DE COVARIANCIES. Calcular la matriu covariancies
```{r,eval=FALSE}
SCR <- t(Y - X %*% betas) %*% (Y - X %*% betas) 
MCE <- SCR/(n-r)
MCE

COV <- (as.numeric(MCE) * XtX.ginv) #matriu de covariances 
```

####COVARIANCIA ENTRE ESTIMADORS
```{r,eval=FALSE}
#Calcul de l'estimació de la covariància entre els estimadors linals òptims
a1 <- c(1,-1,-1) #coeficients d'una de les funcions paramètriques estimables la qual volem calcular la covariància amb un altra
a2 <- c(3,0,-1)
COV.ELO <- a1 %*% COV %*%  a2 
```

####VARIÀNCIA D'UNA 
```{r,eval=FALSE}
#Un exemple d'un calcul d'una variància
3*COV[1,1]+ 2*3*COV[1,2]+ 2*3*COV[1,3]+ 3*COV[2,2]+ 2*3*COV[2,3] + COV[3,3]
#Hem de tindre en compte que la variancia de a+b és: var(a+b)=var(a)+var(b)+2cov(a,v)
#var(ka+mb)=(k^2)var(a)+(m^2)var(b)+(k*m*2)cov(a,b)
```

###MODEL DE REGRESSIÓ. ML i primeres impresions
```{r,eval=FALSE}
bd <- 
attach(bd) #substituir bd pel nom de la base de dades corresponent
Y <- var #substituir var perla variable sobre la qual fer la regressió lineal
```
```{r,eval=FALSE}
g <- lm(Y ~ .) #el . indica que es fa la regressió sobre totes les variables del model(menys Y), hem de treure de la regresió les variables que són discretes on hi ha el punt substituir per (. - nomvardiscreta)

summary(g) #resum de la regressió

# Variància estimada de l'error
summary(g)$sigma^2

#Coeficient de determinació d'ajust
summary(g)$adj.r.squared #com mésproper a 1 -> model millor ajustat

#amb el p-valor del que don summary(g) mirem si és un model significatiu (té sentit fer la regressió) això passa si el p-valor ésmenys de 0.05 (rebutjem hipòtesi nula)


#resum gràfic
par(mfrow = c(2, 2))
plot(g)
par(mfrow = c(1,1))

```

###CONTRAST D'HIPÒTESI ENTRE MODELS

```{r,eval=FALSE}
#Model complet
g <- lm(Y ~ .) #model amb totes dels variables
#es el mateix que fer g <- lm(Y ~ v1 + v2 + ... + vN)

#Model amb seleccionant algunes de les variables regressores (v1,v2,v3,..) no totes
g0 <- lm(Y ~ v1 + v2 + v4) #canviar v1,v2,v4 per les variables que siguin i afegir o treureles que calgui, les variables que no.

#en el cas de que no seleccionem cap variable substituirem v1 + v2 + v4 per 1. esa dir g0 <- lm(Y ~ 1)
```
```{r,eval=FALSE}
#Comparem els dos models amb la taula ANOVA
a<-anova(g0,g) 
a$`Pr(>F)` #si el p valor és méspetit que 0.05 (acceptem hipòtesi alternativa) diem que els dos models són significativament diferents.

#per comparar models mirem si segueixen distribucions iguals això es pot fer amb la taula anova (test F)(comparació de variàncies) o amb el test t (comparació de mitjanes).

```
###INTEVAL DE CONFIANÇA
```{r,eval=FALSE}
confint(g,level = 0.95) #fem el de tot i observem el de la variable que ens interessi.

#si volem fer l'interval de confiança d'uns valors concrets hemde fer:
predict(g, newdata = data.frame(v1 = 0, v2 = 60, v3 = 10, v4 = 11), interval = 'prediction')
```

###DIAGNOSI DEL MODEL

####HOMOCEDASTICITAT. (Variància constant dels errors)

######1.Residuals vs. Fitted i Residuals (en valor absolut) vs. Fitted
```{r,eval=F}
par(mfrow = c(1, 2))
plot(g, which = 1)
plot(fitted(g),abs(residuals(g)),xlab="Fitted",ylab="|Residuals|")
par(mfrow = c(1, 1))

```

######2.Test de Faraway
Test per comprovar si la variància és constant.
```{r,eval=FALSE}
#segurament treballarem amb el faraway
#install.packages("faraway")
library(faraway)
```
```{r,eval=FALSE}
summary(lm(abs(residuals(g)) ~ fitted(g)))# Regressió entre el valor absolut dels residus i els valors ajustats
#


plot(fitted(g),abs(residuals(g)),xlab="Fitted",ylab="|Residuals|") #Gràfic anterior amb la recta que hem trobat ara
abline(summary(lm(abs(residuals(g)) ~ fitted(g)))) #La regressió és significativa, el que implica que seria una recta amb pendent != 0 els errors no són constants.

```

######4.John Fox: test i gràfic
```{r,eval=FALSE}
ncvTest(g)# non-constant error variance test
# p < 0.05  -> Rebutgem homoscedasticitat
```
```{r,eval=FALSE}
#gràfic que podemm adjuntar al test
# plot studentized residuals vs. fitted values
spreadLevelPlot(g)
# Dibuixa una recta de regressió que indica el nivell de constància
# Si no és horitzontal, sinó que té pendent -> els residus van creixent

# Si es rebutja la homoscedasticitat i es violen les suposicions
# de Gauss-Markov, es podria fer alguna transformació de variables
# per intentar solucionar aquest problema.
```
####NORMALITAT DEL MODEL

#####1.QQ-plot
```{r,eval=FALSE}
plot(g, which = 2)
```
#####2.Test formal
```{r,eval=FALSE}
shapiro.test(residuals(g))
# p-value <0.05 -> Rebutgem normalitat
```
####LEVERAGE.Punts amb influència potencial
```{r,eval=FALSE}
leverage <- hatvalues(g)
# criteri: h_ii > 2(k+1)/n on k = nombre de variables
which(leverage > 2(k+1)/n)
# Aquests punts són POTENCIALMENT INFLUENTS
```

####OUTLIERS

Criteri: $|t_i| \ge 2$

```{r,eval=FALSE}
t <-rstudent(g)
which(abs(t)>2)

plot(1:n, t, type = 'h')
abline(h = c(2, 0, -2))
```
Gràficament
```{r,eval=FALSE}
Boxplot(t)
```
Ajust del criteri amb Bonferroni
```{r,eval=FALSE}
outlierTest(g)
```
qqPlot d'ajust als quantils teòrics d'una t-Student
```{r,eval=FALSE}
qqPlot(g, main="QQ Plot", id.n = 3)
```
####PUNTS INFLUENTS. Influència real (sobre els coeficients i sobre les prediccions)

```{r,eval=FALSE}
# Influència real:
  # - sobre els coeficients:
    # - DISTÀNCIA DE COOK
    # - DFBETAS
  # - sobre les prediccions:
    # - DISTÀNCIA DE COOK
    # - DFFITS

# - DISTÀNCIA DE COOK

  C <- cooks.distance(g)
  # criteri: com que no és clar, el millor és dibuixar-ho.
  plot(g, which = 4)


# - DFBETAS
  
  dfbetas(g)
  # limit 2/sqrt(n)
  l <- 2/sqrt(n)

  which(abs((dfbetas(g))[,1])>l)  
  which(abs((dfbetas(g))[,2])>l)  
  which(abs((dfbetas(g))[,3])>l)  
  which(abs((dfbetas(g))[,4])>l)  
  which(abs((dfbetas(g))[,5])>l)  
  
# - DFBETAS
  
  dfbetas(g)
  # limit 2/sqrt(n)
  l <- 2/sqrt(n)

  which(abs((dfbetas(g))[,1])>l)  
  which(abs((dfbetas(g))[,2])>l)  
  which(abs((dfbetas(g))[,3])>l)  
  which(abs((dfbetas(g))[,4])>l)  
  which(abs((dfbetas(g))[,5])>l) 
```




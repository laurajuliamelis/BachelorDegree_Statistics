---
title: "PROVA PARCIAL 1"
author: "LAURA JULIÀ MELIS"
date: "November 13, 2017"
output: html_document
---

## PREGUNTA 1

#### Apartat a.
```{r}
y <- c(2.1, 1.9, 2.0, 2.2, 1.2, 0.8, 1.1, 0.9, 5.1, 5.2, 4.9, 4.9, 7.9, 8.0, 8.2, 7.9)
x <- c(rep(c(1,0,-1, 0),4),rep(c(0,1,0,-1),4),rep(c(1,1,0,0),4),rep(c(2,2,-1,-1),4))
x <- matrix(x,ncol=4, byrow=T)
qr(x)$rank

```
Tenim 4 paràmetres i el rang de la matriu de disseny és 3, llavors per trobar les funcions paramètriques estimables cal resoldre el següent sistema d'equacions:

$(a_1, a_2, a_3, a_4) = \lambda_1(1,0, -1, 0) + \lambda_2(0, 1, 0, -1) + \lambda_3(2, 2, -1, -1)$

De manera que una combinació lineal $\psi = a_1\alpha + a_2\beta + a_3\gamma + a_4\delta$ serà f.p.e si  $a_1= a_2 - a_3 + a_4$.

#### Apartat b.

Mirem si són estimables:

(i) $\alpha = (1,0,0,0): 1 \neq -0+0+0$, per tant, no és estimable.

(ii) $\alpha + \beta = (1,1,0,0): 1 = 1+0+0$, per tant, si és estimable.

(iii) $\alpha + \beta + \gamma + \delta= (1,1,1,1): 1 = -1+1+1$, per tant si és estimable.


Calculem l'estimador MQ:
```{r}
library(MASS)
xtx  <- t(x) %*% x
xtxi <- ginv(xtx)
betas <- xtxi %*% t(x) %*% y 

# alpha + beta
sum(betas[1:2])

# alpha + beta + gamma + delta
betas[1]+ betas[2] + betas[3]+ betas[4]   
```
L'estimador MQ de $\alpha + \beta$ és 5.00625, i el de $\alpha + \beta + \gamma + \delta$ és 7.

#### Apartat c.
```{r}
n <- length(y)
r <- qr(x)$rank
pred  <- x %*% betas   # Calculem les prediccions
resid <- y - pred      # Calculem els residus: diferència entre les observacions i el model
RSS   <- sum(resid^2)  # Residual Sum of Squares
MSE   <- RSS/(n-r)     # ECM = RSS/(n-r) (sigma^2)

# Ens demanen VAR(a)=COV(a,a), VAR(b)=COV(b,b)  I COV(a,b).
# Fórmules: var(a'betas) = sigma^2 a'(X'X)^-a //cov(a'betas, b'betas) = sigma^2 * a'(X'X)^-b
a <- c(1, 1, 0, 0) # a'beta = (1,1,0,0) *(alpha, beta, gamma, delta)
b <- c(2, 1,-1, 0) # b'beta = (2,1,-1,0) *(alpha, beta, gamma, delta)
```


L'estimació de la variància de $\alpha + \beta$ és:
```{r}
varA <- MSE * t(a) %*% xtxi %*% a
varA
```



L'estimació de la variància de $2\alpha + \beta - \gamma$ és:
```{r}
varB <- MSE * t(b) %*% xtxi %*% b
varB
```



La covariància entre els dos estimadors és:
```{r}
cov <- MSE * t(a) %*% xtxi %*% b
cov
```


#### Apartat d.

Primer contrast ($H_0: \alpha + \beta =5$):
```{r}
t.est <- as.numeric((sum(betas[1:2]) - 5)/sqrt(varA))
pt(t.est, n - r, lower.tail = F) * 2
```
Com que p-valor = 0.9235789 > 0.05, acceptem la hipòtesi nul·la.



Segon contrast ($H_0: \alpha + \beta =5,  2\alpha + \beta - \gamma =7$):
```{r}
#Mourem els paràmetres de lloc, escrivint-los on les observacions, i obtenint així:
y0 <- y - c(rep(2, 4), rep(5, 4), rep(5, 4), rep(12, 4)) #Noves observacions.
x0 <- c(rep(c(0, 0), 4), rep(c(-1, -1), 4), rep(c(0, 0), 4), rep(c(-1, -1), 4)) # Nova matriu de disseny.

# Com que qr(x0)$rank =1 , simplifiquem la matriu:
x0 <- c(rep(0, 4), rep(-1, 4), rep(0, 4), rep(-1, 4))

beta0 <- solve(t(x0) %*% x0) %*% t(x0) %*% y0
residus0 <- y0 - x0 * as.vector(beta0)
RSS0 <- sum(residus0^2) # suma de quadrats residual de la hipòtesi nul·la

#Test F
f.est <- ((RSS0 - RSS)/2)/(RSS/(n - r))
pf(f.est, 2, n - r, lower.tail = F)

```

Com que p-valor = 0.8556013 > 0.05, acceptem la hipòtesi nul·la.


## PREGUNTA 2

Selecció de les dades:
```{r}
library(HistData)
data(GaltonFamilies)
GaltonFills <- GaltonFamilies[GaltonFamilies$gender=="male", ]
GaltonFilles <- GaltonFamilies[GaltonFamilies$gender=="female", ] # selecció a l'atzar d'un únic fill o filla per familia
set.seed(123)
sel.unic <- function(df) {
rownames(df) <- 1:dim(df)[1]
ff <- function(x) as.numeric(sample(as.character(which(df$family==x)),1))
ind <- sapply(unique(df$family),ff)
df[ind, ]
}
# data.frames per treballar
GaltonFills1 <- sel.unic(GaltonFills)
GaltonFilles1 <- sel.unic(GaltonFilles)
```

#### Apartat a.
```{r}
# Gràfic de dispersió:
with(GaltonFills1, plot(father, childHeight))

# Primera regressió lineal:
g <- lm(childHeight ~ father, data = GaltonFills1)
with(GaltonFills1, plot(father, childHeight))
abline(g)

# Anàlisi gràfica dels residus:
plot(g, which = 1)
```

Cal indicar tres observacions atípiques, amb residus molt grans, la 119, la 216 i la 245.

#### Apartat b.
```{r}
# Estimació beta0 i beta1
coef(g) 

# Estimació sigma2
ss <- summary(g)
ss$sigma^2 

# Coeficient de determinació:
ss$r.squared
```
Estimacions: $\beta_0 = 38.4308 ,  \beta_1 =0.446$ i $\sigma^2 =5.173$

La regressió és significativa perquè l'estadístic F és significatiu, ja que F= sg$fstatistic[1]= 44.39114 i el seu pvalor és inferior 
a 0.05 (3.281e-10). Això significa que la variable father te una influència significativa (relevant o important) en childHeight.


#### Apartat c.
```{r}
# Gràficament:
plot(g, which = 2)

# Numèricament. Test de normalitat de Shoapiro:
shapiro.test(residuals(g))
```

No hi ha cap raó per dubtar de la normalitat dels residus, ja que pvalor > 0.05 i els punt en el qq plot s'observen molt majoritàriamen per sobre de la línia.

La normalitat no garanteix de mínima variància dels estimadors lineals. 

El teorema de Gauss-Markov diu que el métode dels mínims quadrats amb les tres hipòtesis de Gauss-Markov (no cal la normalitat) és el que garantitza la mínima variància dels estimadors dels paràmetres o f.p.e.


#### Apartat d.


```{r}
# IC de beta0 i beta1.
confint(g, level = 0.9)

#IC de sigma2.
RSS <- sum(ss$residuals^2)
c(RSS/qchisq(0.95, ss$df[2]), RSS/qchisq(0.05, ss$df[2]))
```



#### Apartat e.

Primer contrast d'hipòtesi ($H_0: \beta_1 = 0.495$ amb les dades pare/fill de Galton):
```{r}
g0 <- lm(childHeight ~ offset(I(0.495 * father)), data = GaltonFills1)
anova(g0, g)
```

No es pot rebutjar la hipòtesi nul·la ja que p valor = 0.4653.


Segon contras d'hipòtesi ($H_0: \beta_0 = 104.023, \beta_1 = 0.450$ amb les dades mare/fill de Galton):
```{r}
gmare <- lm(childHeight ~ mother, data = GaltonFills1)
g0 <- lm(childHeight ~ 0 + offset(I(104.023/2.54 + 0.45 * mother)), data = GaltonFills1)
anova(g0, gmare)
```
Rebutjem la hipòtesi nula ja que pvalor(0.0014) < 0.05.

El coeficient d'intercepció ($\beta_0$) en les dades de Galton les unitats són in, i en les dades espanyoles, centímetres. Pel que fa a $\beta_1$, no té unitats en ningún dels dos casos, ja que tant les $in$ com els $cm$ es simplifiquen.

#### Apartat f.

Sí que seria millor fer servir les altures de pare i mare en el model, però no prenguent com a variable regressora una mitjana de l'altura del pare i de la mare, com va fer Galton, ja que segur que no tenen la mateixa influència el pare i la mare en l'altura dels seus fills o filles. 
Per això, seria més convenient considerar l'altura del pare i l'altura de la mare com dues variables regressores diferents, i contriuir un model de regressió múltiple.

#### Apartat g.
```{r}
gpm<- lm(childHeight ~ father + mother, data = GaltonFills1)
predict(gpm, newdata = data.frame(father = 70, mother= 63), interval = "confidence", level = 0.99)
```


#### Apartat h.

El problema que es pot tenir al considerar en el mateix model tots els fills homes de la mateixa família és que podria haver dependència entres les dades i, segons la tercera condició de Gauss-Markov ($E(\epsilon_i\epsilon_j) = 0 , \forall i \neq j$, les observacions han de ser incorrelacionades.



